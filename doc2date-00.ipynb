{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2date: A Case Study in Document Regression\n",
    "# Introduction\n",
    "\n",
    "[Document classification](https://en.wikipedia.org/wiki/Document_classification) is a common application of machine learning techniques. Examples include [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis), the classification of texts into a (typically small) number of moods (such as \"positive\" and \"negative\"); as well as authorship attribution in [stylometry](https://en.wikipedia.org/wiki/Stylometry), in which texts are grouped according to their original author.\n",
    "\n",
    "Unsupersived learning methods have also been applied to the analysis of documents. For instance, [doc2vec](https://arxiv.org/pdf/1405.4053v2.pdf) is a dimensionality reduction technique that extends [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) to entire documents.\n",
    "\n",
    "But what about document *regression*? In this and later notebooks, we investigate the problem of learning the date of a publication from the text contained therein. Since the target space, a range of years, can be viewed as a continuum, this problem presents a natural test case for applying regression techniques to document analysis. These methods may be of use in author document regression problems. A particular example that comes to mind is author age prediction, which could be of use in market segmentation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "[Part 1](doc2date-01.ipynb) focuses on **exploratory data analysis** of yearly aggregate word counts from the following [HathiTrust](https://htrc.illinois.edu/) research center data set: [Word frequencies in English-Language Literature, 1700-1922](https://wiki.htrc.illinois.edu/display/COM/Word+Frequencies+in+English-Language+Literature%2C+1700-1922). The entire dataset contains word frequencies compiled from 178 831 volumes published over the period 1700-1922; this data is separated by content (fiction, poetry, and drama) and we focus on the largest category: fiction. In part 2, we focus solely on yearly aggregate word counts, which are easier and faster to analyze.\n",
    "\n",
    "In [Part 2](doc2date-02.ipynb) we make use of individual volume data from the HTRC data set. We leverage the insights derived in Part 1 towards **building and deploying a model** suitable for making predictions on raw text.\n",
    "\n",
    "[Part 3](doc2date-03.ipynb) looks at potential improvements to the models considered in part 2 that could be obtained by working with a richer set of data. Specifically, we work with the [Gutenberg corpus](https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html), which contains not only raw text rather than just word counts. There are two new difficulties with this data set: it is not preprocessed and it is unlabeled. Although the second problem can be seen as an aspect of the first, it is the more serious problem. In part 3, we consider methods for automatically labeling data as well as sophisticated **machine learning methods** for dealing with partially labeled data.\n",
    "\n",
    "## Start reading\n",
    "\n",
    "[Part 1](doc2date-01.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
